## Предисловие

В завершение нашего разговора о парсинге погрузимся на уровень ниже. В этом задании вам предстоит реализовать собственный асинхронный парсер почти без использования сторонних библиотек. Цель этого задания заключается в том, чтобы вы лучше разобрались в современных подходах к веб-парсингу, посмотрев на то, как это реализовано "от" и "до" на Python. Вы создадите полноценный мини-проект, который в будущем можно будет использовать в тех случаях, когда слишком простыми инструментами не обойтись, а слишком продвинутые избыточны (или на их использование установлены какие-то ограничения). Перед выполнением этого задания мы рекомендуем вам:

- Изучить устройство [Scrapy](https://github.com/scrapy/scrapy/tree/master/scrapy). Конечно, это очень богатая библиотека, которую совместными усилиями разрабатывают сотни людей. Поэтому достаточно по ней просто пробежаться, обратив внимание на API ключевых инструментов, таких как [Spider](https://github.com/scrapy/scrapy/blob/master/scrapy/spiders/__init__.py).
- Вспомните про `asyncio`. Обратите внимание на [asyncio.LifoQueue](https://docs.python.org/3/library/asyncio-queue.html#asyncio.LifoQueue).
- Продумайте шаги, которые должен сделать парсер, чтобы выполнить задание. Разбейте действия парсера на сценарий. Это поможет лучше продумать структуру кода и не смешивать в одну функции с разными зонами ответственности.


## Задание

Вам необходимо написать собственный парсер для извлечения данных с сайта [Steam](https://store.steampowered.com/search/?filter=popularnew). Парсер должен быть **асинхронным**. Вам нужно сделать N поисковых запросов (например, `strategy`) и пройтись по первым K страницам, если столько страниц есть, вытащив следующие данные для всех найденных игр:

- Название
- Цена
- Рейтинг текстом (поле "Все обзоры")
- Разработчик
- Жанр (все, если несколько)
- Дата выхода

Если страниц меньше K, то парсер должен идти до последней страницы. Количество запросов, сами запросы и количество страниц для парсинга должны задаваться как константы, чтобы их можно было легко изменить при последующих запусках скрипта. Steam не блокирует парсинг, но рекомендуется делать задержку между запросами. Результаты парсинга нужно сохранить в SQLite базу данных `results.db` в таблицу `games`.

> Обратите внимание, что по ссылке выше игры подгружаются **динамически**, но Playwright или Selenium использовать не надо. Прочитайте про query-параметры, посмотрите примеры. Подумайте, какой query-параметр нужно добавить в URL, чтобы избежать динамической подгрузки контента и открыть страницу со стандартным пагинатором.

При большом желании весь код этого задания можно уместить в одной функции. Такой код может работать, но он не будет **качественным** и его будет очень трудно **тестировать**. Мы очень рекомендуем подумать над структурой функций или классов, правильно декомпозировать код и в конце порадоваться тому, какое изящество у вас получилось. Вот небольшие рекомендации, что вы можете предусмотреть:

- Класс для описания протокола данных
- Класс / функции для работы с базой данных
- Контекстный менеджер для загрузки нужных страниц (получение HTML)
- Класс / функция для парсинга страниц (извлечение из HTML)


## Критерии оценивания

- Наличие всего результата: 10
- Наличие только одного запроса: -4
- Отсутствие фильтрации: -2
- Не ходит по страницам: -2
- Не все данные вытащены: -2
- Красивый код, выполнены рекомендации: +2

Но не больше 10.
